{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering my interests in Hacker News with NLP\n",
    "# Part II: Scrapping Hacker News favorities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scrapping we use Selenium instead of simple \"requests\" because the major of Hacker News aggregated pages are JS content generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from typing import Union, Tuple, List\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import multiprocessing\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROME_DRIVER_PATH = os.path.join(os.getcwd(), 'contrib/chromedriver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic class for scrap with Selenium and Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScraperWebJS:\n",
    "    \n",
    "    prepared = False\n",
    "    \n",
    "    def prepare(self):\n",
    "        \"\"\"\n",
    "        Prepare the headless browser for a scrap session\n",
    "        \"\"\"\n",
    "        if self.prepared:\n",
    "            return\n",
    "        self.prepared = True\n",
    "        opts = Options()\n",
    "        opts.add_argument('--headless')\n",
    "        opts.add_argument('windows-size=1920,1080')\n",
    "        self.drv = webdriver.Chrome(executable_path=CHROME_DRIVER_PATH, options=opts)\n",
    "        self.drv.implicitly_wait(10)\n",
    "    \n",
    "    def scrape(self, url: str) -> Union[str, None]:\n",
    "        \"\"\"\n",
    "        Scrape the \"url\" using selenium with Chrome backend\n",
    "        \"\"\"\n",
    "        if not self.prepared:\n",
    "            self.prepare()\n",
    "        try:\n",
    "            self.drv.get(url)\n",
    "        except Exception as err:\n",
    "            self.finish()\n",
    "            return None\n",
    "        return self.drv.page_source\n",
    "\n",
    "    def finish(self):\n",
    "        \"\"\"\n",
    "        Finish scrapping session\n",
    "        \"\"\"\n",
    "        if self.prepared:\n",
    "            self.drv.quit()\n",
    "            self.prepared = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the favorite URLS to scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hn_favs = json.loads(open('hn_favs.json', 'r').read())\n",
    "urls = [x['url'] for x in hn_favs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41810180bae475abfbf579e1e5e4a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=207.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrapper = ScraperWebJS()\n",
    "scraped_urls = []\n",
    "for url in tqdm(urls):\n",
    "    try:\n",
    "        content = scrapper.scrape(url)\n",
    "    except Exception as err:\n",
    "        print(f\"Error can't scrape {url} => {err}\")\n",
    "        continue\n",
    "    if content is None:\n",
    "        continue\n",
    "    scraped_urls.append({'url': url, 'content': content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Pandas to \"dump\" de scrapped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saved the scrapped content to process them in the last part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(scraped_urls)\n",
    "df.to_pickle('hn_favs_scrapped.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
